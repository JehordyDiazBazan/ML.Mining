{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyfP6Gozs++KPO7X30vVK7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JehordyDiazBazan/ML.Mining/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzC9PJhJHq9a",
        "outputId": "a9519345-ca8f-480b-8e84-25a7fca86914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 8524.5869 - mae: 85.0303 - mse: 8524.5869 - val_loss: 9064.3457 - val_mae: 87.6193 - val_mse: 9064.3457\n",
            "Epoch 2/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8346.9268 - mae: 84.4417 - mse: 8346.9268 - val_loss: 8890.9268 - val_mae: 86.6173 - val_mse: 8890.9268\n",
            "Epoch 3/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8232.1582 - mae: 84.1871 - mse: 8232.1582 - val_loss: 8602.6318 - val_mae: 84.9324 - val_mse: 8602.6318\n",
            "Epoch 4/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8142.6064 - mae: 83.0661 - mse: 8142.6064 - val_loss: 8141.9072 - val_mae: 82.1915 - val_mse: 8141.9072\n",
            "Epoch 5/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7536.9189 - mae: 79.6476 - mse: 7536.9189 - val_loss: 7444.3423 - val_mae: 78.0552 - val_mse: 7444.3423\n",
            "Epoch 6/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6962.2183 - mae: 75.6447 - mse: 6962.2183 - val_loss: 6448.7822 - val_mae: 72.0098 - val_mse: 6448.7822\n",
            "Epoch 7/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5682.4888 - mae: 67.6745 - mse: 5682.4888 - val_loss: 5134.2915 - val_mae: 63.5879 - val_mse: 5134.2915\n",
            "Epoch 8/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4380.0249 - mae: 57.9942 - mse: 4380.0249 - val_loss: 3576.7485 - val_mae: 51.9109 - val_mse: 3576.7485\n",
            "Epoch 9/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2785.9475 - mae: 44.9643 - mse: 2785.9475 - val_loss: 2019.0547 - val_mae: 37.7163 - val_mse: 2019.0547\n",
            "Epoch 10/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1578.9564 - mae: 31.6444 - mse: 1578.9564 - val_loss: 921.7111 - val_mae: 25.6102 - val_mse: 921.7111\n",
            "Epoch 11/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 737.4662 - mae: 21.5001 - mse: 737.4662 - val_loss: 650.1383 - val_mae: 20.0102 - val_mse: 650.1383\n",
            "Epoch 12/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 719.4925 - mae: 22.4147 - mse: 719.4925 - val_loss: 658.3868 - val_mae: 19.7897 - val_mse: 658.3868\n",
            "Epoch 13/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 611.5510 - mae: 20.1429 - mse: 611.5510 - val_loss: 435.2045 - val_mae: 15.7863 - val_mse: 435.2045\n",
            "Epoch 14/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 439.1052 - mae: 16.9396 - mse: 439.1052 - val_loss: 261.0013 - val_mae: 12.7855 - val_mse: 261.0013\n",
            "Epoch 15/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 253.8998 - mae: 12.5429 - mse: 253.8998 - val_loss: 231.2052 - val_mae: 12.4785 - val_mse: 231.2052\n",
            "Epoch 16/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 249.6045 - mae: 12.5436 - mse: 249.6045 - val_loss: 208.4287 - val_mae: 12.0044 - val_mse: 208.4287\n",
            "Epoch 17/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 210.2938 - mae: 11.9909 - mse: 210.2938 - val_loss: 154.5983 - val_mae: 10.1555 - val_mse: 154.5983\n",
            "Epoch 18/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 163.5981 - mae: 10.1540 - mse: 163.5981 - val_loss: 114.5212 - val_mae: 8.4084 - val_mse: 114.5212\n",
            "Epoch 19/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 139.3969 - mae: 9.3618 - mse: 139.3969 - val_loss: 104.9832 - val_mae: 8.0236 - val_mse: 104.9832\n",
            "Epoch 20/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 149.7584 - mae: 9.8148 - mse: 149.7584 - val_loss: 98.7766 - val_mae: 7.8521 - val_mse: 98.7766\n",
            "Epoch 21/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 144.4458 - mae: 9.6260 - mse: 144.4458 - val_loss: 91.5735 - val_mae: 7.5019 - val_mse: 91.5735\n",
            "Epoch 22/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 141.8152 - mae: 9.2194 - mse: 141.8152 - val_loss: 92.4342 - val_mae: 7.4514 - val_mse: 92.4342\n",
            "Epoch 23/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 126.3221 - mae: 9.0269 - mse: 126.3221 - val_loss: 91.4166 - val_mae: 7.3901 - val_mse: 91.4166\n",
            "Epoch 24/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 122.4055 - mae: 8.8342 - mse: 122.4055 - val_loss: 86.7005 - val_mae: 7.2701 - val_mse: 86.7005\n",
            "Epoch 25/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 111.0246 - mae: 8.5014 - mse: 111.0246 - val_loss: 83.4516 - val_mae: 7.1796 - val_mse: 83.4516\n",
            "Epoch 26/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 119.8324 - mae: 8.7688 - mse: 119.8324 - val_loss: 82.6734 - val_mae: 7.1202 - val_mse: 82.6734\n",
            "Epoch 27/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 120.6729 - mae: 8.8098 - mse: 120.6729 - val_loss: 80.6082 - val_mae: 7.0468 - val_mse: 80.6082\n",
            "Epoch 28/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 114.6090 - mae: 8.5750 - mse: 114.6090 - val_loss: 78.9162 - val_mae: 6.9904 - val_mse: 78.9162\n",
            "Epoch 29/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 101.2023 - mae: 7.8442 - mse: 101.2023 - val_loss: 78.7538 - val_mae: 6.9783 - val_mse: 78.7538\n",
            "Epoch 30/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 107.4018 - mae: 8.1883 - mse: 107.4018 - val_loss: 78.1198 - val_mae: 6.9474 - val_mse: 78.1198\n",
            "Epoch 31/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 94.6630 - mae: 7.6957 - mse: 94.6630 - val_loss: 76.8497 - val_mae: 6.8896 - val_mse: 76.8497\n",
            "Epoch 32/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 115.7994 - mae: 8.5382 - mse: 115.7994 - val_loss: 76.1299 - val_mae: 6.8988 - val_mse: 76.1299\n",
            "Epoch 33/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 103.2353 - mae: 8.1825 - mse: 103.2353 - val_loss: 74.2662 - val_mae: 6.8485 - val_mse: 74.2662\n",
            "Epoch 34/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 102.2903 - mae: 8.0050 - mse: 102.2903 - val_loss: 73.6505 - val_mae: 6.8529 - val_mse: 73.6505\n",
            "Epoch 35/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 105.7678 - mae: 8.0074 - mse: 105.7678 - val_loss: 73.0694 - val_mae: 6.8429 - val_mse: 73.0694\n",
            "Epoch 36/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 104.3357 - mae: 8.0149 - mse: 104.3357 - val_loss: 72.6466 - val_mae: 6.8304 - val_mse: 72.6466\n",
            "Epoch 37/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 102.7178 - mae: 8.2008 - mse: 102.7178 - val_loss: 73.5347 - val_mae: 6.8558 - val_mse: 73.5347\n",
            "Epoch 38/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 102.3146 - mae: 7.8259 - mse: 102.3146 - val_loss: 73.1320 - val_mae: 6.8091 - val_mse: 73.1320\n",
            "Epoch 39/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 103.0805 - mae: 7.7490 - mse: 103.0805 - val_loss: 71.7860 - val_mae: 6.7168 - val_mse: 71.7860\n",
            "Epoch 40/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 84.8757 - mae: 7.3589 - mse: 84.8757 - val_loss: 71.4340 - val_mae: 6.7024 - val_mse: 71.4340\n",
            "Epoch 41/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 93.2548 - mae: 7.6541 - mse: 93.2548 - val_loss: 71.1509 - val_mae: 6.6659 - val_mse: 71.1509\n",
            "Epoch 42/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 110.5899 - mae: 8.3024 - mse: 110.5899 - val_loss: 71.8997 - val_mae: 6.6826 - val_mse: 71.8997\n",
            "Epoch 43/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 103.6716 - mae: 8.1882 - mse: 103.6716 - val_loss: 71.5279 - val_mae: 6.6825 - val_mse: 71.5279\n",
            "Epoch 44/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 87.0551 - mae: 7.4319 - mse: 87.0551 - val_loss: 71.3799 - val_mae: 6.6624 - val_mse: 71.3799\n",
            "Epoch 45/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 99.1445 - mae: 7.8045 - mse: 99.1445 - val_loss: 71.4952 - val_mae: 6.6712 - val_mse: 71.4952\n",
            "Epoch 46/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 91.8529 - mae: 7.6316 - mse: 91.8529 - val_loss: 70.9935 - val_mae: 6.6760 - val_mse: 70.9935\n",
            "Epoch 47/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 102.0635 - mae: 7.9773 - mse: 102.0635 - val_loss: 71.0091 - val_mae: 6.6668 - val_mse: 71.0091\n",
            "Epoch 48/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 92.4434 - mae: 7.5829 - mse: 92.4434 - val_loss: 70.9452 - val_mae: 6.6615 - val_mse: 70.9452\n",
            "Epoch 49/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 91.4580 - mae: 7.6139 - mse: 91.4580 - val_loss: 71.1234 - val_mae: 6.7197 - val_mse: 71.1234\n",
            "Epoch 50/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 91.0455 - mae: 7.7249 - mse: 91.0455 - val_loss: 72.0127 - val_mae: 6.7861 - val_mse: 72.0127\n",
            "Epoch 51/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 85.4478 - mae: 7.5676 - mse: 85.4478 - val_loss: 71.8881 - val_mae: 6.7878 - val_mse: 71.8881\n",
            "Epoch 52/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 82.7392 - mae: 7.2446 - mse: 82.7392 - val_loss: 71.6943 - val_mae: 6.7910 - val_mse: 71.6943\n",
            "Epoch 53/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 92.6488 - mae: 7.5805 - mse: 92.6488 - val_loss: 71.3905 - val_mae: 6.7905 - val_mse: 71.3905\n",
            "Epoch 54/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 93.1610 - mae: 7.8406 - mse: 93.1610 - val_loss: 71.0368 - val_mae: 6.7600 - val_mse: 71.0368\n",
            "Epoch 55/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 94.2450 - mae: 7.5990 - mse: 94.2450 - val_loss: 70.7667 - val_mae: 6.7219 - val_mse: 70.7667\n",
            "Epoch 56/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 94.6027 - mae: 7.8649 - mse: 94.6027 - val_loss: 70.2976 - val_mae: 6.7272 - val_mse: 70.2976\n",
            "Epoch 57/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 81.7832 - mae: 7.3186 - mse: 81.7832 - val_loss: 69.4079 - val_mae: 6.7029 - val_mse: 69.4079\n",
            "Epoch 58/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 80.9132 - mae: 7.1895 - mse: 80.9132 - val_loss: 68.2262 - val_mae: 6.6570 - val_mse: 68.2262\n",
            "Epoch 59/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 89.9417 - mae: 7.5217 - mse: 89.9417 - val_loss: 68.0010 - val_mae: 6.6144 - val_mse: 68.0010\n",
            "Epoch 60/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 87.4635 - mae: 7.5580 - mse: 87.4635 - val_loss: 68.6792 - val_mae: 6.6578 - val_mse: 68.6792\n",
            "Epoch 61/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 81.2931 - mae: 7.2843 - mse: 81.2931 - val_loss: 70.0513 - val_mae: 6.7164 - val_mse: 70.0513\n",
            "Epoch 62/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 88.4368 - mae: 7.5052 - mse: 88.4368 - val_loss: 69.9418 - val_mae: 6.6588 - val_mse: 69.9418\n",
            "Epoch 63/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 92.3462 - mae: 7.7735 - mse: 92.3462 - val_loss: 69.3706 - val_mae: 6.5722 - val_mse: 69.3706\n",
            "Epoch 64/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 80.6258 - mae: 7.2468 - mse: 80.6258 - val_loss: 67.6559 - val_mae: 6.4919 - val_mse: 67.6559\n",
            "Epoch 65/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 90.8387 - mae: 7.5454 - mse: 90.8387 - val_loss: 65.7169 - val_mae: 6.4556 - val_mse: 65.7169\n",
            "Epoch 66/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 77.1144 - mae: 7.0850 - mse: 77.1144 - val_loss: 65.4023 - val_mae: 6.4946 - val_mse: 65.4023\n",
            "Epoch 67/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 76.7466 - mae: 7.0573 - mse: 76.7466 - val_loss: 66.6454 - val_mae: 6.5915 - val_mse: 66.6454\n",
            "Epoch 68/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 76.1121 - mae: 7.0058 - mse: 76.1121 - val_loss: 68.2519 - val_mae: 6.6988 - val_mse: 68.2519\n",
            "Epoch 69/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 87.3028 - mae: 7.4908 - mse: 87.3028 - val_loss: 69.2938 - val_mae: 6.7819 - val_mse: 69.2938\n",
            "Epoch 70/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 86.8850 - mae: 7.6471 - mse: 86.8850 - val_loss: 70.5172 - val_mae: 6.7998 - val_mse: 70.5172\n",
            "Epoch 71/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 79.4043 - mae: 7.2453 - mse: 79.4043 - val_loss: 68.9335 - val_mae: 6.6416 - val_mse: 68.9335\n",
            "Epoch 72/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 90.0654 - mae: 7.5724 - mse: 90.0654 - val_loss: 66.5014 - val_mae: 6.5382 - val_mse: 66.5014\n",
            "Epoch 73/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 85.3815 - mae: 7.4742 - mse: 85.3815 - val_loss: 63.7751 - val_mae: 6.3953 - val_mse: 63.7751\n",
            "Epoch 74/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 77.6488 - mae: 6.9356 - mse: 77.6488 - val_loss: 61.5367 - val_mae: 6.2432 - val_mse: 61.5367\n",
            "Epoch 75/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 77.6930 - mae: 6.9675 - mse: 77.6930 - val_loss: 60.6725 - val_mae: 6.1938 - val_mse: 60.6725\n",
            "Epoch 76/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 82.1860 - mae: 7.2980 - mse: 82.1860 - val_loss: 60.7423 - val_mae: 6.2584 - val_mse: 60.7423\n",
            "Epoch 77/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 70.5364 - mae: 6.6932 - mse: 70.5364 - val_loss: 61.2236 - val_mae: 6.2732 - val_mse: 61.2236\n",
            "Epoch 78/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 77.1808 - mae: 6.9819 - mse: 77.1808 - val_loss: 62.8293 - val_mae: 6.2989 - val_mse: 62.8293\n",
            "Epoch 79/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 78.8096 - mae: 7.2617 - mse: 78.8096 - val_loss: 61.1581 - val_mae: 6.2494 - val_mse: 61.1581\n",
            "Epoch 80/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 83.5721 - mae: 7.1389 - mse: 83.5721 - val_loss: 59.7235 - val_mae: 6.2123 - val_mse: 59.7235\n",
            "Epoch 81/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 74.7177 - mae: 7.0619 - mse: 74.7177 - val_loss: 59.5695 - val_mae: 6.2068 - val_mse: 59.5695\n",
            "Epoch 82/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 70.9076 - mae: 6.8245 - mse: 70.9076 - val_loss: 60.5030 - val_mae: 6.3034 - val_mse: 60.5030\n",
            "Epoch 83/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 80.2592 - mae: 7.3344 - mse: 80.2592 - val_loss: 61.5786 - val_mae: 6.3620 - val_mse: 61.5786\n",
            "Epoch 84/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 57.2180 - mae: 5.8801 - mse: 57.2180 - val_loss: 61.4437 - val_mae: 6.3420 - val_mse: 61.4437\n",
            "Epoch 85/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 70.5903 - mae: 6.5525 - mse: 70.5903 - val_loss: 62.4444 - val_mae: 6.3984 - val_mse: 62.4444\n",
            "Epoch 86/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 77.0183 - mae: 7.0669 - mse: 77.0183 - val_loss: 62.6415 - val_mae: 6.4278 - val_mse: 62.6415\n",
            "Epoch 87/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 74.8796 - mae: 6.6702 - mse: 74.8796 - val_loss: 62.4178 - val_mae: 6.4342 - val_mse: 62.4178\n",
            "Epoch 88/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 78.1219 - mae: 6.9588 - mse: 78.1219 - val_loss: 62.1369 - val_mae: 6.3927 - val_mse: 62.1369\n",
            "Epoch 89/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 74.5196 - mae: 6.6205 - mse: 74.5196 - val_loss: 60.5821 - val_mae: 6.2467 - val_mse: 60.5821\n",
            "Epoch 90/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 71.0507 - mae: 6.7428 - mse: 71.0507 - val_loss: 58.9290 - val_mae: 6.1854 - val_mse: 58.9290\n",
            "Epoch 91/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 62.8517 - mae: 6.3777 - mse: 62.8517 - val_loss: 59.1443 - val_mae: 6.2492 - val_mse: 59.1443\n",
            "Epoch 92/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 76.7369 - mae: 7.0347 - mse: 76.7369 - val_loss: 59.0474 - val_mae: 6.2719 - val_mse: 59.0474\n",
            "Epoch 93/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 76.7369 - mae: 6.9985 - mse: 76.7369 - val_loss: 59.6028 - val_mae: 6.3108 - val_mse: 59.6028\n",
            "Epoch 94/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 67.8633 - mae: 6.5374 - mse: 67.8633 - val_loss: 59.5918 - val_mae: 6.2024 - val_mse: 59.5918\n",
            "Epoch 95/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 76.3601 - mae: 6.9425 - mse: 76.3601 - val_loss: 59.8189 - val_mae: 6.2350 - val_mse: 59.8189\n",
            "Epoch 96/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 70.3660 - mae: 6.8051 - mse: 70.3660 - val_loss: 61.5119 - val_mae: 6.4502 - val_mse: 61.5119\n",
            "Epoch 97/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 74.9965 - mae: 6.8932 - mse: 74.9965 - val_loss: 60.0890 - val_mae: 6.3952 - val_mse: 60.0890\n",
            "Epoch 98/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 68.9175 - mae: 6.6393 - mse: 68.9175 - val_loss: 59.2051 - val_mae: 6.1850 - val_mse: 59.2051\n",
            "Epoch 99/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 77.2407 - mae: 7.0219 - mse: 77.2407 - val_loss: 56.9519 - val_mae: 6.0710 - val_mse: 56.9519\n",
            "Epoch 100/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 72.1565 - mae: 6.8914 - mse: 72.1565 - val_loss: 55.4928 - val_mae: 6.0595 - val_mse: 55.4928\n",
            "Epoch 101/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 67.6293 - mae: 6.6427 - mse: 67.6293 - val_loss: 56.9286 - val_mae: 6.1180 - val_mse: 56.9286\n",
            "Epoch 102/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 60.8277 - mae: 6.1076 - mse: 60.8277 - val_loss: 58.8333 - val_mae: 6.1827 - val_mse: 58.8333\n",
            "Epoch 103/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 70.5646 - mae: 6.8137 - mse: 70.5646 - val_loss: 59.5453 - val_mae: 6.1814 - val_mse: 59.5453\n",
            "Epoch 104/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 66.6169 - mae: 6.4089 - mse: 66.6169 - val_loss: 60.4510 - val_mae: 6.2277 - val_mse: 60.4510\n",
            "Epoch 105/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 78.4212 - mae: 7.0896 - mse: 78.4212 - val_loss: 57.6520 - val_mae: 6.1409 - val_mse: 57.6520\n",
            "Epoch 106/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 60.0971 - mae: 6.2693 - mse: 60.0971 - val_loss: 55.8687 - val_mae: 6.0732 - val_mse: 55.8687\n",
            "Epoch 107/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 64.2110 - mae: 6.3728 - mse: 64.2110 - val_loss: 54.9526 - val_mae: 5.9878 - val_mse: 54.9526\n",
            "Epoch 108/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 70.6659 - mae: 6.8926 - mse: 70.6659 - val_loss: 55.9956 - val_mae: 6.0127 - val_mse: 55.9956\n",
            "Epoch 109/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 59.2554 - mae: 6.1213 - mse: 59.2554 - val_loss: 55.7460 - val_mae: 6.0557 - val_mse: 55.7460\n",
            "Epoch 110/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 65.8233 - mae: 6.6029 - mse: 65.8233 - val_loss: 55.5645 - val_mae: 5.9952 - val_mse: 55.5645\n",
            "Epoch 111/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 63.1828 - mae: 6.2028 - mse: 63.1828 - val_loss: 55.8814 - val_mae: 6.0711 - val_mse: 55.8814\n",
            "Epoch 112/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 62.8331 - mae: 6.2727 - mse: 62.8331 - val_loss: 57.2019 - val_mae: 6.1463 - val_mse: 57.2019\n",
            "Epoch 113/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 64.6498 - mae: 6.5541 - mse: 64.6498 - val_loss: 56.3684 - val_mae: 6.1026 - val_mse: 56.3684\n",
            "Epoch 114/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 63.2066 - mae: 6.3206 - mse: 63.2066 - val_loss: 55.0140 - val_mae: 6.0060 - val_mse: 55.0140\n",
            "Epoch 115/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 62.1436 - mae: 6.3314 - mse: 62.1436 - val_loss: 53.8410 - val_mae: 5.8194 - val_mse: 53.8410\n",
            "Epoch 116/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 61.9534 - mae: 6.0219 - mse: 61.9534 - val_loss: 56.5303 - val_mae: 5.9618 - val_mse: 56.5303\n",
            "Epoch 117/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 58.0330 - mae: 5.8681 - mse: 58.0330 - val_loss: 59.0877 - val_mae: 6.1603 - val_mse: 59.0877\n",
            "Epoch 118/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 57.7029 - mae: 6.0779 - mse: 57.7029 - val_loss: 57.1179 - val_mae: 6.0861 - val_mse: 57.1179\n",
            "Epoch 119/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 61.6048 - mae: 5.9354 - mse: 61.6048 - val_loss: 56.5652 - val_mae: 6.0151 - val_mse: 56.5652\n",
            "Epoch 120/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 62.6106 - mae: 6.3617 - mse: 62.6106 - val_loss: 54.1088 - val_mae: 5.9398 - val_mse: 54.1088\n",
            "Epoch 121/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 65.2769 - mae: 6.4148 - mse: 65.2769 - val_loss: 55.9566 - val_mae: 6.0962 - val_mse: 55.9566\n",
            "Epoch 122/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 63.9058 - mae: 6.3364 - mse: 63.9058 - val_loss: 53.6720 - val_mae: 5.9094 - val_mse: 53.6720\n",
            "Epoch 123/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 58.0018 - mae: 5.9802 - mse: 58.0018 - val_loss: 57.4014 - val_mae: 5.9959 - val_mse: 57.4014\n",
            "Epoch 124/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 55.8925 - mae: 5.8461 - mse: 55.8925 - val_loss: 54.2158 - val_mae: 5.8874 - val_mse: 54.2158\n",
            "Epoch 125/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 55.4345 - mae: 6.0424 - mse: 55.4345 - val_loss: 56.0027 - val_mae: 6.1061 - val_mse: 56.0027\n",
            "Epoch 126/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 70.7871 - mae: 6.5634 - mse: 70.7871 - val_loss: 53.0830 - val_mae: 5.8205 - val_mse: 53.0830\n",
            "Epoch 127/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 54.4433 - mae: 5.9042 - mse: 54.4433 - val_loss: 52.3104 - val_mae: 5.7264 - val_mse: 52.3104\n",
            "Epoch 128/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 53.7913 - mae: 5.7218 - mse: 53.7913 - val_loss: 51.0263 - val_mae: 5.8228 - val_mse: 51.0263\n",
            "Epoch 129/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 55.5019 - mae: 5.9816 - mse: 55.5019 - val_loss: 49.8794 - val_mae: 5.7225 - val_mse: 49.8794\n",
            "Epoch 130/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 57.1876 - mae: 6.1252 - mse: 57.1876 - val_loss: 47.9761 - val_mae: 5.5411 - val_mse: 47.9761\n",
            "Epoch 131/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 58.2509 - mae: 5.8900 - mse: 58.2509 - val_loss: 46.9812 - val_mae: 5.5426 - val_mse: 46.9812\n",
            "Epoch 132/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 53.2069 - mae: 5.5760 - mse: 53.2069 - val_loss: 49.1195 - val_mae: 5.7015 - val_mse: 49.1195\n",
            "Epoch 133/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 50.4836 - mae: 5.5625 - mse: 50.4836 - val_loss: 52.6363 - val_mae: 5.9481 - val_mse: 52.6363\n",
            "Epoch 134/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 51.2851 - mae: 5.6224 - mse: 51.2851 - val_loss: 50.4700 - val_mae: 5.7679 - val_mse: 50.4700\n",
            "Epoch 135/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 60.7872 - mae: 6.1043 - mse: 60.7872 - val_loss: 51.3210 - val_mae: 5.8053 - val_mse: 51.3210\n",
            "Epoch 136/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 55.1793 - mae: 5.8154 - mse: 55.1793 - val_loss: 51.5269 - val_mae: 5.7002 - val_mse: 51.5269\n",
            "Epoch 137/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 50.7280 - mae: 5.6789 - mse: 50.7280 - val_loss: 49.5936 - val_mae: 5.6144 - val_mse: 49.5936\n",
            "Epoch 138/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 48.2007 - mae: 5.4271 - mse: 48.2007 - val_loss: 50.2926 - val_mae: 5.7304 - val_mse: 50.2926\n",
            "Epoch 139/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 53.7180 - mae: 5.8238 - mse: 53.7180 - val_loss: 45.8070 - val_mae: 5.4847 - val_mse: 45.8070\n",
            "Epoch 140/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 46.1542 - mae: 5.3898 - mse: 46.1542 - val_loss: 45.4548 - val_mae: 5.4577 - val_mse: 45.4548\n",
            "Epoch 141/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 52.2783 - mae: 5.7679 - mse: 52.2783 - val_loss: 47.0171 - val_mae: 5.6052 - val_mse: 47.0171\n",
            "Epoch 142/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 54.8957 - mae: 5.7098 - mse: 54.8957 - val_loss: 49.3991 - val_mae: 5.7419 - val_mse: 49.3991\n",
            "Epoch 143/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 57.0020 - mae: 5.8968 - mse: 57.0020 - val_loss: 47.8529 - val_mae: 5.5496 - val_mse: 47.8529\n",
            "Epoch 144/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 54.4770 - mae: 5.6696 - mse: 54.4770 - val_loss: 46.7697 - val_mae: 5.3787 - val_mse: 46.7697\n",
            "Epoch 145/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 52.8528 - mae: 5.6272 - mse: 52.8528 - val_loss: 45.9301 - val_mae: 5.4556 - val_mse: 45.9301\n",
            "Epoch 146/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 44.2818 - mae: 5.3019 - mse: 44.2818 - val_loss: 44.4093 - val_mae: 5.3394 - val_mse: 44.4093\n",
            "Epoch 147/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 54.2498 - mae: 5.9436 - mse: 54.2498 - val_loss: 44.5421 - val_mae: 5.3300 - val_mse: 44.5421\n",
            "Epoch 148/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 47.8506 - mae: 5.5174 - mse: 47.8506 - val_loss: 45.8094 - val_mae: 5.4171 - val_mse: 45.8094\n",
            "Epoch 149/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 55.3681 - mae: 5.6875 - mse: 55.3681 - val_loss: 44.8713 - val_mae: 5.2994 - val_mse: 44.8713\n",
            "Epoch 150/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 60.0083 - mae: 6.3005 - mse: 60.0083 - val_loss: 44.1943 - val_mae: 5.2013 - val_mse: 44.1943\n",
            "Epoch 151/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 54.5403 - mae: 5.8410 - mse: 54.5403 - val_loss: 45.6000 - val_mae: 5.3925 - val_mse: 45.6000\n",
            "Epoch 152/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 49.4935 - mae: 5.5744 - mse: 49.4935 - val_loss: 46.3749 - val_mae: 5.5609 - val_mse: 46.3749\n",
            "Epoch 153/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 54.5966 - mae: 5.8562 - mse: 54.5966 - val_loss: 44.6462 - val_mae: 5.3994 - val_mse: 44.6462\n",
            "Epoch 154/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 49.2155 - mae: 5.4180 - mse: 49.2155 - val_loss: 44.5668 - val_mae: 5.2844 - val_mse: 44.5668\n",
            "Epoch 155/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 48.9786 - mae: 5.6100 - mse: 48.9786 - val_loss: 43.2681 - val_mae: 5.3401 - val_mse: 43.2681\n",
            "Epoch 156/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 43.6743 - mae: 5.3858 - mse: 43.6743 - val_loss: 43.4641 - val_mae: 5.3716 - val_mse: 43.4641\n",
            "Epoch 157/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 47.6026 - mae: 5.4192 - mse: 47.6026 - val_loss: 44.4981 - val_mae: 5.4808 - val_mse: 44.4981\n",
            "Epoch 158/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 51.0788 - mae: 5.5130 - mse: 51.0788 - val_loss: 42.3766 - val_mae: 5.2523 - val_mse: 42.3766\n",
            "Epoch 159/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 44.8788 - mae: 5.4016 - mse: 44.8788 - val_loss: 41.9840 - val_mae: 5.2375 - val_mse: 41.9840\n",
            "Epoch 160/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 55.9098 - mae: 5.9749 - mse: 55.9098 - val_loss: 42.1421 - val_mae: 5.2717 - val_mse: 42.1421\n",
            "Epoch 161/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 49.9008 - mae: 5.4844 - mse: 49.9008 - val_loss: 39.5610 - val_mae: 4.9193 - val_mse: 39.5610\n",
            "Epoch 162/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 46.7480 - mae: 5.4541 - mse: 46.7480 - val_loss: 39.7332 - val_mae: 4.9771 - val_mse: 39.7332\n",
            "Epoch 163/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 47.2511 - mae: 5.5104 - mse: 47.2511 - val_loss: 42.3719 - val_mae: 5.2560 - val_mse: 42.3719\n",
            "Epoch 164/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 48.0149 - mae: 5.3425 - mse: 48.0149 - val_loss: 40.2645 - val_mae: 5.1430 - val_mse: 40.2645\n",
            "Epoch 165/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 48.5282 - mae: 5.4602 - mse: 48.5282 - val_loss: 38.4498 - val_mae: 4.9489 - val_mse: 38.4498\n",
            "Epoch 166/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 58.5715 - mae: 5.9948 - mse: 58.5715 - val_loss: 38.8146 - val_mae: 5.0747 - val_mse: 38.8146\n",
            "Epoch 167/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 41.1463 - mae: 4.8129 - mse: 41.1463 - val_loss: 38.9286 - val_mae: 5.0604 - val_mse: 38.9286\n",
            "Epoch 168/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 54.7316 - mae: 5.7560 - mse: 54.7316 - val_loss: 39.4893 - val_mae: 5.0975 - val_mse: 39.4893\n",
            "Epoch 169/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 42.9921 - mae: 5.2443 - mse: 42.9921 - val_loss: 39.6146 - val_mae: 5.1444 - val_mse: 39.6146\n",
            "Epoch 170/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 43.7086 - mae: 5.2559 - mse: 43.7086 - val_loss: 40.5980 - val_mae: 5.1963 - val_mse: 40.5980\n",
            "Epoch 171/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 42.8530 - mae: 5.2508 - mse: 42.8530 - val_loss: 39.8629 - val_mae: 5.0647 - val_mse: 39.8629\n",
            "Epoch 172/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 46.3640 - mae: 5.3821 - mse: 46.3640 - val_loss: 40.2893 - val_mae: 5.0739 - val_mse: 40.2893\n",
            "Epoch 173/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 44.9557 - mae: 5.2001 - mse: 44.9557 - val_loss: 39.2416 - val_mae: 5.0028 - val_mse: 39.2416\n",
            "Epoch 174/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 46.8803 - mae: 5.3004 - mse: 46.8803 - val_loss: 40.1910 - val_mae: 5.1918 - val_mse: 40.1910\n",
            "Epoch 175/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 44.8367 - mae: 5.1652 - mse: 44.8367 - val_loss: 40.2843 - val_mae: 5.2374 - val_mse: 40.2843\n",
            "Epoch 176/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 48.8056 - mae: 5.5645 - mse: 48.8056 - val_loss: 37.0345 - val_mae: 4.9184 - val_mse: 37.0345\n",
            "Epoch 177/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 45.1510 - mae: 5.2796 - mse: 45.1510 - val_loss: 36.7343 - val_mae: 4.8677 - val_mse: 36.7343\n",
            "Epoch 178/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 51.1533 - mae: 5.5563 - mse: 51.1533 - val_loss: 38.8742 - val_mae: 5.0292 - val_mse: 38.8742\n",
            "Epoch 179/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 41.3748 - mae: 4.9766 - mse: 41.3748 - val_loss: 37.1356 - val_mae: 4.7590 - val_mse: 37.1356\n",
            "Epoch 180/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 59.6292 - mae: 5.9413 - mse: 59.6292 - val_loss: 38.1987 - val_mae: 4.9812 - val_mse: 38.1987\n",
            "Epoch 181/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 47.3397 - mae: 5.5360 - mse: 47.3397 - val_loss: 40.2582 - val_mae: 5.1936 - val_mse: 40.2582\n",
            "Epoch 182/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 43.8998 - mae: 5.0136 - mse: 43.8998 - val_loss: 37.7758 - val_mae: 5.0258 - val_mse: 37.7758\n",
            "Epoch 183/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 45.7681 - mae: 5.2823 - mse: 45.7681 - val_loss: 37.3013 - val_mae: 4.9317 - val_mse: 37.3013\n",
            "Epoch 184/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 45.1600 - mae: 5.0798 - mse: 45.1600 - val_loss: 36.8373 - val_mae: 4.9790 - val_mse: 36.8373\n",
            "Epoch 185/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 45.8347 - mae: 5.1315 - mse: 45.8347 - val_loss: 35.7019 - val_mae: 4.7954 - val_mse: 35.7019\n",
            "Epoch 186/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 39.6867 - mae: 5.1093 - mse: 39.6867 - val_loss: 35.9585 - val_mae: 4.8128 - val_mse: 35.9585\n",
            "Epoch 187/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 39.3291 - mae: 4.9540 - mse: 39.3291 - val_loss: 38.4464 - val_mae: 5.0009 - val_mse: 38.4464\n",
            "Epoch 188/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 48.0620 - mae: 5.6008 - mse: 48.0620 - val_loss: 38.5275 - val_mae: 5.0557 - val_mse: 38.5275\n",
            "Epoch 189/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 42.7863 - mae: 5.0173 - mse: 42.7863 - val_loss: 39.1603 - val_mae: 5.0531 - val_mse: 39.1603\n",
            "Epoch 190/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 44.0365 - mae: 5.1691 - mse: 44.0365 - val_loss: 37.4913 - val_mae: 4.9369 - val_mse: 37.4913\n",
            "Epoch 191/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 41.3437 - mae: 4.9740 - mse: 41.3437 - val_loss: 35.0108 - val_mae: 4.6930 - val_mse: 35.0108\n",
            "Epoch 192/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 38.2283 - mae: 4.8160 - mse: 38.2283 - val_loss: 34.9227 - val_mae: 4.6538 - val_mse: 34.9227\n",
            "Epoch 193/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 43.9186 - mae: 5.1258 - mse: 43.9186 - val_loss: 35.6555 - val_mae: 4.7000 - val_mse: 35.6555\n",
            "Epoch 194/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 40.0483 - mae: 4.9574 - mse: 40.0483 - val_loss: 36.2877 - val_mae: 4.8183 - val_mse: 36.2877\n",
            "Epoch 195/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 46.2060 - mae: 5.3138 - mse: 46.2060 - val_loss: 34.8138 - val_mae: 4.8125 - val_mse: 34.8138\n",
            "Epoch 196/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 48.4784 - mae: 5.5637 - mse: 48.4784 - val_loss: 36.4983 - val_mae: 5.0002 - val_mse: 36.4983\n",
            "Epoch 197/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 39.0689 - mae: 4.9251 - mse: 39.0689 - val_loss: 36.9364 - val_mae: 4.9668 - val_mse: 36.9364\n",
            "Epoch 198/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 48.8110 - mae: 5.4824 - mse: 48.8110 - val_loss: 39.3668 - val_mae: 4.9365 - val_mse: 39.3668\n",
            "Epoch 199/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 44.3346 - mae: 5.2560 - mse: 44.3346 - val_loss: 35.5779 - val_mae: 4.6057 - val_mse: 35.5779\n",
            "Epoch 200/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 42.2715 - mae: 4.8188 - mse: 42.2715 - val_loss: 37.2334 - val_mae: 4.9613 - val_mse: 37.2334\n",
            "Epoch 201/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 47.0333 - mae: 5.2948 - mse: 47.0333 - val_loss: 33.6416 - val_mae: 4.5604 - val_mse: 33.6416\n",
            "Epoch 202/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 32.5943 - mae: 4.3896 - mse: 32.5943 - val_loss: 33.2525 - val_mae: 4.5717 - val_mse: 33.2525\n",
            "Epoch 203/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 44.1819 - mae: 5.1744 - mse: 44.1819 - val_loss: 38.7436 - val_mae: 5.0266 - val_mse: 38.7436\n",
            "Epoch 204/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 46.4376 - mae: 5.2940 - mse: 46.4376 - val_loss: 34.1974 - val_mae: 4.6832 - val_mse: 34.1974\n",
            "Epoch 205/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 44.2129 - mae: 4.9533 - mse: 44.2129 - val_loss: 33.9356 - val_mae: 4.6709 - val_mse: 33.9356\n",
            "Epoch 206/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 45.7993 - mae: 5.1529 - mse: 45.7993 - val_loss: 34.0782 - val_mae: 4.5531 - val_mse: 34.0782\n",
            "Epoch 207/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 40.5436 - mae: 4.9952 - mse: 40.5436 - val_loss: 34.1725 - val_mae: 4.6393 - val_mse: 34.1725\n",
            "Epoch 208/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 41.4115 - mae: 5.0845 - mse: 41.4115 - val_loss: 31.6129 - val_mae: 4.4743 - val_mse: 31.6129\n",
            "Epoch 209/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 35.8473 - mae: 4.9360 - mse: 35.8473 - val_loss: 30.7779 - val_mae: 4.3442 - val_mse: 30.7779\n",
            "Epoch 210/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 44.1890 - mae: 5.0367 - mse: 44.1890 - val_loss: 31.0211 - val_mae: 4.4135 - val_mse: 31.0211\n",
            "Epoch 211/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 38.9182 - mae: 4.9946 - mse: 38.9182 - val_loss: 31.7018 - val_mae: 4.5277 - val_mse: 31.7018\n",
            "Epoch 212/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 47.0333 - mae: 5.1943 - mse: 47.0333 - val_loss: 29.7109 - val_mae: 4.2891 - val_mse: 29.7109\n",
            "Epoch 213/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 38.8435 - mae: 4.9446 - mse: 38.8435 - val_loss: 29.2721 - val_mae: 4.2982 - val_mse: 29.2721\n",
            "Epoch 214/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 39.4098 - mae: 4.9069 - mse: 39.4098 - val_loss: 30.7235 - val_mae: 4.4807 - val_mse: 30.7235\n",
            "Epoch 215/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 46.8840 - mae: 5.3617 - mse: 46.8840 - val_loss: 33.1647 - val_mae: 4.6315 - val_mse: 33.1647\n",
            "Epoch 216/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 44.8461 - mae: 4.9537 - mse: 44.8461 - val_loss: 35.0363 - val_mae: 4.7071 - val_mse: 35.0363\n",
            "Epoch 217/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 34.3825 - mae: 4.5829 - mse: 34.3825 - val_loss: 34.4175 - val_mae: 4.6735 - val_mse: 34.4175\n",
            "Epoch 218/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 35.5229 - mae: 4.6732 - mse: 35.5229 - val_loss: 33.3186 - val_mae: 4.5803 - val_mse: 33.3186\n",
            "Epoch 219/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 42.3108 - mae: 5.0251 - mse: 42.3108 - val_loss: 31.2465 - val_mae: 4.3879 - val_mse: 31.2465\n",
            "Epoch 220/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 36.3336 - mae: 4.7178 - mse: 36.3336 - val_loss: 34.4346 - val_mae: 4.7598 - val_mse: 34.4346\n",
            "Epoch 221/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 33.5011 - mae: 4.5359 - mse: 33.5011 - val_loss: 32.2152 - val_mae: 4.5623 - val_mse: 32.2152\n",
            "Epoch 222/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 40.1619 - mae: 4.9307 - mse: 40.1619 - val_loss: 30.7501 - val_mae: 4.3967 - val_mse: 30.7501\n",
            "Epoch 223/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 39.6735 - mae: 4.7943 - mse: 39.6735 - val_loss: 33.1452 - val_mae: 4.7153 - val_mse: 33.1452\n",
            "Epoch 224/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 35.8877 - mae: 4.7472 - mse: 35.8877 - val_loss: 33.5534 - val_mae: 4.7892 - val_mse: 33.5534\n",
            "Epoch 225/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 46.0007 - mae: 5.1520 - mse: 46.0007 - val_loss: 30.3880 - val_mae: 4.2870 - val_mse: 30.3880\n",
            "Epoch 226/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 36.7021 - mae: 4.7852 - mse: 36.7021 - val_loss: 30.6698 - val_mae: 4.4069 - val_mse: 30.6698\n",
            "Epoch 227/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 40.3875 - mae: 4.7323 - mse: 40.3875 - val_loss: 30.7838 - val_mae: 4.3762 - val_mse: 30.7838\n",
            "Epoch 228/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 30.8662 - mae: 4.4684 - mse: 30.8662 - val_loss: 30.0142 - val_mae: 4.3232 - val_mse: 30.0142\n",
            "Epoch 229/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 30.1727 - mae: 4.2697 - mse: 30.1727 - val_loss: 29.6923 - val_mae: 4.2852 - val_mse: 29.6923\n",
            "Epoch 230/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 38.8903 - mae: 4.9607 - mse: 38.8903 - val_loss: 29.7961 - val_mae: 4.3512 - val_mse: 29.7961\n",
            "Epoch 231/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 40.2216 - mae: 5.0302 - mse: 40.2216 - val_loss: 30.4365 - val_mae: 4.4005 - val_mse: 30.4365\n",
            "Epoch 232/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 32.8878 - mae: 4.2496 - mse: 32.8878 - val_loss: 31.3510 - val_mae: 4.4846 - val_mse: 31.3510\n",
            "Epoch 233/250\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 43.2760 - mae: 4.9781 - mse: 43.2760 - val_loss: 32.0770 - val_mae: 4.5870 - val_mse: 32.0770\n",
            "\n",
            "=== MÉTRICAS DE TEST ===\n",
            "{\n",
            "  \"MAE\": 4.808811664581299,\n",
            "  \"RMSE\": 6.215948192542554,\n",
            "  \"R2\": 0.9672783613204956\n",
            "}\n",
            "\n",
            "✅ Resultados en 'artifacts/':\n",
            "- metrics_overview.png (métricas en UNA sola ventana)\n",
            "- training_history.csv\n",
            "- modelo_tf_mina.keras\n",
            "- predicciones_test.csv\n",
            "- Ton_vs_{Cu/Au/Ag}.png\n",
            "- NSR_vs_{Cu, Ton}.png (si hay NSR)\n",
            "- grado_tonnage_{Ley}_grado_tonnage_combined.png + _curve.csv (con línea de cut-off si aplica)\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ================================================================\n",
        "# PLAN DE MINADO + TENSORFLOW (todo en una sola ventana)\n",
        "# - Entrenamiento (regresión o clasificación)\n",
        "# - Métricas combinadas en UNA gráfica (loss + val_loss, MAE/Accuracy + val_*)\n",
        "# - Curvas Ton vs Ley, NSR vs Ton/Ley\n",
        "# - Curvas Grado–Tonnage (doble eje) + CSV\n",
        "# - LÍNEA VERTICAL de cut-off económico (auto o manual)\n",
        "# ================================================================\n",
        "\n",
        "# =========================\n",
        "# CONFIG (edita aquí)\n",
        "# =========================\n",
        "DATA_PATH = \"plantilla_minado.csv\"   # CSV / XLSX / ODS\n",
        "TARGET    = \"NSR_USD_t\"                # p.ej. \"NSR_USD_t\" o \"Clase\"\n",
        "SHEET     = None                       # nombre de hoja si aplica (xlsx/ods)\n",
        "ID_COLS   = [\"BlockID\", \"ID\", \"Id\", \"index\"]\n",
        "\n",
        "# Columnas típicas para gráficas / corte económico (ajusta si difieren)\n",
        "TON_COL  = \"Ton\"\n",
        "CU_COL   = \"Cu_pct\"\n",
        "AU_COL   = \"Au_gpt\"\n",
        "AG_COL   = \"Ag_gpt\"\n",
        "NSR_COL  = \"NSR_USD_t\"                 # NSR por tonelada (para cut-off económico auto)\n",
        "COST_COL = \"Cost_USD_t\"                # Costo por tonelada (para cut-off económico auto)\n",
        "\n",
        "# Cut-offs manuales (si no hay columnas NSR/Costo o prefieres fijarlos tú)\n",
        "# clave = nombre de columna de ley en tu data; valor = cut-off de ley\n",
        "MANUAL_CUTOFFS = {\n",
        "    # Ejemplos:\n",
        "    # \"Cu_pct\": 0.25,   # 0.25% Cu\n",
        "    # \"Au_gpt\": 0.3,    # 0.3 g/t Au\n",
        "    # \"Ag_gpt\": 5.0,    # 5 g/t Ag\n",
        "}\n",
        "\n",
        "# Entrenamiento\n",
        "EPOCHS    = 250\n",
        "BATCH     = 64\n",
        "VAL_SPLIT = 0.2\n",
        "\n",
        "# Salidas\n",
        "ARTIFACTS_DIR = \"artifacts\"\n",
        "\n",
        "# =========================\n",
        "# IMPORTS Y UTILIDADES\n",
        "# =========================\n",
        "import os, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "\n",
        "def read_table(path, sheet=None):\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    if ext in [\".csv\", \".txt\"]:\n",
        "        return pd.read_csv(path)\n",
        "    elif ext in [\".xlsx\", \".xls\"]:\n",
        "        return pd.read_excel(path, sheet_name=sheet, engine=\"openpyxl\")\n",
        "    elif ext in [\".ods\"]:\n",
        "        return pd.read_excel(path, sheet_name=sheet, engine=\"odf\")\n",
        "    else:\n",
        "        raise ValueError(f\"Formato no soportado: {ext}\")\n",
        "\n",
        "def clean_table(df):\n",
        "    df = df.dropna(axis=1, how=\"all\").dropna(axis=0, how=\"all\").drop_duplicates()\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def split_numeric_categorical(df, exclude):\n",
        "    feats = [c for c in df.columns if c not in exclude]\n",
        "    num_cols = [c for c in feats if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    cat_cols = [c for c in feats if c not in num_cols]\n",
        "    return num_cols, cat_cols\n",
        "\n",
        "def build_matrix(df, target, id_like, cat_max_unique=50):\n",
        "    data = df[df[target].notna()].copy()\n",
        "    exclude = set(id_like + [target])\n",
        "    num_cols, cat_cols = split_numeric_categorical(data, exclude)\n",
        "\n",
        "    kept_cat = []\n",
        "    for c in cat_cols:\n",
        "        nuni = data[c].nunique(dropna=True)\n",
        "        if 1 < nuni <= cat_max_unique:\n",
        "            kept_cat.append(c)\n",
        "\n",
        "    if num_cols:\n",
        "        data[num_cols] = data[num_cols].apply(lambda s: s.fillna(s.median()))\n",
        "    for c in kept_cat:\n",
        "        data[c] = data[c].astype(str).fillna(\"NA\")\n",
        "\n",
        "    if kept_cat:\n",
        "        data = pd.get_dummies(data, columns=kept_cat, drop_first=True)\n",
        "\n",
        "    X_cols = [c for c in data.columns if c != target and c not in id_like]\n",
        "    X = data[X_cols].values.astype(\"float32\")\n",
        "    y = data[target].values\n",
        "    return X, y, X_cols\n",
        "\n",
        "def infer_problem_type(df, target):\n",
        "    if str(df[target].dtype) == \"object\" or target.lower() in [\"clase\", \"class\", \"tipo\"]:\n",
        "        return \"classification\"\n",
        "    return \"regression\"\n",
        "\n",
        "def encode_labels(y, problem_type):\n",
        "    if problem_type == \"classification\":\n",
        "        classes = sorted(pd.Series(y).astype(str).unique())\n",
        "        mapping = {cls: i for i, cls in enumerate(classes)}\n",
        "        return pd.Series(y).astype(str).map(mapping).values.astype(\"int64\"), mapping\n",
        "    return pd.to_numeric(y, errors=\"coerce\").astype(\"float32\"), None\n",
        "\n",
        "def split_data(X, y, test_size=0.2):\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=42, shuffle=True)\n",
        "\n",
        "def build_model(input_dim, problem_type):\n",
        "    inputs = keras.Input(shape=(input_dim,), name=\"features\")\n",
        "    norm = layers.Normalization(name=\"norm\")\n",
        "    x = norm(inputs)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    if problem_type == \"regression\":\n",
        "        outputs = layers.Dense(1, name=\"target\")(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\", \"mse\"])\n",
        "    else:\n",
        "        outputs = layers.Dense(1, activation=None, name=\"logits\")(x)  # se ajusta si K>2\n",
        "        model = keras.Model(inputs, outputs)\n",
        "    return model, norm\n",
        "\n",
        "def compile_for_classification(model, y_train):\n",
        "    n_classes = int(np.max(y_train)) + 1\n",
        "    if n_classes == 2:\n",
        "        model.compile(optimizer=\"adam\",\n",
        "                      loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                      metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")])\n",
        "    else:\n",
        "        x = model.layers[-2].output\n",
        "        outputs = layers.Dense(n_classes, activation=\"softmax\", name=\"target\")(x)\n",
        "        model = keras.Model(model.input, outputs)\n",
        "        model.compile(optimizer=\"adam\",\n",
        "                      loss=\"sparse_categorical_crossentropy\",\n",
        "                      metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def save_history_csv(history, out_csv_path):\n",
        "    hist_df = pd.DataFrame({k: [float(x) for x in v] for k, v in history.history.items()})\n",
        "    hist_df.index.name = \"epoch\"\n",
        "    hist_df.to_csv(out_csv_path)\n",
        "\n",
        "def plot_metrics_combined(history, problem_type, outpath, test_report=None):\n",
        "    \"\"\"\n",
        "    UNA SOLA VENTANA:\n",
        "    - Eje Y izquierdo: loss / val_loss\n",
        "    - Eje Y derecho: MAE/val_MAE (+RMSE/val_RMSE si hay MSE) o Accuracy/val_Accuracy\n",
        "    - Anota métricas de TEST en la figura\n",
        "    \"\"\"\n",
        "    h = history.history\n",
        "    epochs = range(1, len(next(iter(h.values()))) + 1)\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(9, 5))\n",
        "    # Loss\n",
        "    ax1.plot(epochs, h.get(\"loss\", []), label=\"loss\")\n",
        "    if \"val_loss\" in h:\n",
        "        ax1.plot(epochs, h[\"val_loss\"], label=\"val_loss\")\n",
        "    ax1.set_xlabel(\"Épocas\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper left\")\n",
        "\n",
        "    # Métrica\n",
        "    ax2 = ax1.twinx()\n",
        "    if problem_type == \"regression\":\n",
        "        if \"mae\" in h:\n",
        "            ax2.plot(epochs, h[\"mae\"], linestyle=\"--\", label=\"mae\")\n",
        "        if \"val_mae\" in h:\n",
        "            ax2.plot(epochs, h[\"val_mae\"], linestyle=\"--\", label=\"val_mae\")\n",
        "        if \"mse\" in h:\n",
        "            rmse = [math.sqrt(float(v)) for v in h[\"mse\"]]\n",
        "            ax2.plot(epochs, rmse, linestyle=\":\", label=\"rmse\")\n",
        "        if \"val_mse\" in h:\n",
        "            val_rmse = [math.sqrt(float(v)) for v in h[\"val_mse\"]]\n",
        "            ax2.plot(epochs, val_rmse, linestyle=\":\", label=\"val_rmse\")\n",
        "        ax2.set_ylabel(\"MAE / RMSE\")\n",
        "    else:\n",
        "        acc_key = \"accuracy\" if \"accuracy\" in h else (\"binary_accuracy\" if \"binary_accuracy\" in h else None)\n",
        "        val_acc_key = \"val_accuracy\" if \"val_accuracy\" in h else (\"val_binary_accuracy\" if \"val_binary_accuracy\" in h else None)\n",
        "        if acc_key:\n",
        "            ax2.plot(epochs, h[acc_key], linestyle=\"--\", label=acc_key)\n",
        "        if val_acc_key:\n",
        "            ax2.plot(epochs, h[val_acc_key], linestyle=\"--\", label=val_acc_key)\n",
        "        ax2.set_ylabel(\"Accuracy\")\n",
        "\n",
        "    # Leyendas combinadas\n",
        "    l1, lab1 = ax1.get_legend_handles_labels()\n",
        "    l2, lab2 = ax2.get_legend_handles_labels()\n",
        "    ax2.legend(l1 + l2, lab1 + lab2, loc=\"lower right\")\n",
        "\n",
        "    # Anotación de métricas de TEST\n",
        "    if test_report:\n",
        "        txt = \"TEST:\\n\" + \"\\n\".join([f\"{k}: {v:.4f}\" for k, v in test_report.items()])\n",
        "        ax1.text(0.98, 0.02, txt, transform=ax1.transAxes,\n",
        "                 ha=\"right\", va=\"bottom\",\n",
        "                 bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.85))\n",
        "\n",
        "    plt.title(\"Métricas de entrenamiento (una sola ventana)\")\n",
        "    plt.tight_layout()\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
        "    plt.savefig(outpath, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def safe_scatter(x, y, xlabel, ylabel, title, outpath, s=None):\n",
        "    plt.figure()\n",
        "    plt.scatter(x, y, alpha=0.6, s=s)\n",
        "    plt.xlabel(xlabel); plt.ylabel(ylabel); plt.title(title)\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outpath, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def grado_tonnage_curve(df, grade_col, ton_col, steps=100):\n",
        "    d = df[[grade_col, ton_col]].dropna()\n",
        "    if d.empty: return None\n",
        "    g, t = d[grade_col].astype(float), d[ton_col].astype(float)\n",
        "    gmin, gmax = float(np.nanmin(g)), float(np.nanmax(g))\n",
        "    if not np.isfinite(gmin) or not np.isfinite(gmax) or gmax <= gmin:\n",
        "        return None\n",
        "    cuts = np.linspace(gmin, gmax, steps)\n",
        "    rows = []\n",
        "    for c in cuts:\n",
        "        mask = g >= c\n",
        "        if mask.any():\n",
        "            ton = float(t[mask].sum())\n",
        "            mean_grade = float(np.average(g[mask], weights=t[mask]))\n",
        "            rows.append((c, ton, mean_grade))\n",
        "        else:\n",
        "            rows.append((c, 0.0, np.nan))\n",
        "    return pd.DataFrame(rows, columns=[\"cutoff\", \"tonnage_above\", \"grade_mean_above\"])\n",
        "\n",
        "def compute_economic_cutoff(df, grade_col, ton_col, nsr_col, cost_col, steps=120):\n",
        "    \"\"\"\n",
        "    Cálculo auto de cut-off económico (si hay NSR y Costo):\n",
        "    Devuelve el MENOR cutoff (sobre la malla de cortes) tal que\n",
        "    promedio_ponderado_ton( NSR - Costo | ley >= cutoff ) >= 0\n",
        "    \"\"\"\n",
        "    if not ({grade_col, ton_col, nsr_col, cost_col} <= set(df.columns)):\n",
        "        return None\n",
        "    d = df[[grade_col, ton_col, nsr_col, cost_col]].dropna()\n",
        "    if d.empty: return None\n",
        "    g = d[grade_col].astype(float).values\n",
        "    t = d[ton_col].astype(float).values\n",
        "    nsr = d[nsr_col].astype(float).values\n",
        "    cost = d[cost_col].astype(float).values\n",
        "    margin = nsr - cost\n",
        "    gmin, gmax = float(np.nanmin(g)), float(np.nanmax(g))\n",
        "    if not np.isfinite(gmin) or not np.isfinite(gmax) or gmax <= gmin:\n",
        "        return None\n",
        "    cuts = np.linspace(gmin, gmax, steps)\n",
        "    feasible = []\n",
        "    for c in cuts:\n",
        "        mask = g >= c\n",
        "        if np.any(mask):\n",
        "            w = t[mask]\n",
        "            m = margin[mask]\n",
        "            # promedio ponderado por tonelaje\n",
        "            avg_margin = float(np.sum(m * w) / np.sum(w))\n",
        "            if avg_margin >= 0:\n",
        "                feasible.append(c)\n",
        "    if feasible:\n",
        "        return float(np.min(feasible))\n",
        "    return None\n",
        "\n",
        "def plot_grado_tonnage_combined(curve_df, grade_label, outprefix, vline=None):\n",
        "    \"\"\"\n",
        "    Curva Grado–Tonnage combinada (doble eje) + (opcional) línea vertical en cut-off\n",
        "    \"\"\"\n",
        "    if curve_df is None or curve_df.empty:\n",
        "        return\n",
        "    fig, ax1 = plt.subplots(figsize=(8,5))\n",
        "    ax1.set_xlabel(f\"Corte {grade_label}\")\n",
        "    ax1.set_ylabel(\"Tonnage ≥ corte\", color=\"tab:blue\")\n",
        "    ax1.plot(curve_df[\"cutoff\"], curve_df[\"tonnage_above\"], color=\"tab:blue\", label=\"Tonnage ≥ corte\")\n",
        "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel(f\"Ley media ≥ corte ({grade_label})\", color=\"tab:red\")\n",
        "    ax2.plot(curve_df[\"cutoff\"], curve_df[\"grade_mean_above\"], color=\"tab:red\", linestyle=\"--\",\n",
        "             label=f\"Ley media ≥ corte ({grade_label})\")\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
        "\n",
        "    # Línea vertical de cut-off (si aplica)\n",
        "    if vline is not None:\n",
        "        ax1.axvline(float(vline), color=\"tab:green\", linestyle=\":\", linewidth=2)\n",
        "        ax1.text(float(vline), ax1.get_ylim()[1]*0.95,\n",
        "                 f\" Cut-off = {vline:.4g}\",\n",
        "                 color=\"tab:green\", rotation=90, va=\"top\", ha=\"left\",\n",
        "                 bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
        "\n",
        "    plt.title(f\"Curva Grado–Tonnage ({grade_label})\")\n",
        "    fig.tight_layout()\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "    plt.savefig(f\"{outprefix}_grado_tonnage_combined.png\", dpi=150)\n",
        "    plt.close()\n",
        "    # CSV\n",
        "    curve_df.to_csv(f\"{outprefix}_curve.csv\", index=False)\n",
        "\n",
        "# =========================\n",
        "# EJECUCIÓN\n",
        "# =========================\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"No existe DATA_PATH: {DATA_PATH}. Ajusta la ruta en CONFIG o sube el archivo.\")\n",
        "\n",
        "# Cargar y limpiar\n",
        "df = clean_table(read_table(DATA_PATH, sheet=SHEET))\n",
        "\n",
        "# Asegurar TARGET\n",
        "if TARGET not in df.columns:\n",
        "    lower = {c.lower(): c for c in df.columns}\n",
        "    tgt = None\n",
        "    for cand in [\"nsr_usd_t\", \"nsr\", \"valor_neto\", \"valor\", \"net smelter return\", \"clase\", \"class\", \"tipo\"]:\n",
        "        if cand in lower:\n",
        "            tgt = lower[cand]; break\n",
        "    if tgt is None:\n",
        "        raise ValueError(f\"No se encontró la columna objetivo '{TARGET}' ni candidatos conocidos.\")\n",
        "    TARGET = tgt\n",
        "\n",
        "# Dataset (one-hot, imputación…)\n",
        "X, y_raw, X_cols = build_matrix(df, TARGET, id_like=ID_COLS, cat_max_unique=50)\n",
        "problem_type = infer_problem_type(df, TARGET)\n",
        "y, class_mapping = encode_labels(y_raw, problem_type)\n",
        "\n",
        "# Split y modelo\n",
        "X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2)\n",
        "model, norm = build_model(X_train.shape[1], problem_type)\n",
        "norm.adapt(X_train)\n",
        "if problem_type == \"classification\":\n",
        "    model = compile_for_classification(model, y_train)\n",
        "\n",
        "# Entrenamiento\n",
        "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=VAL_SPLIT,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Evaluación test\n",
        "if problem_type == \"regression\":\n",
        "    y_pred = model.predict(X_test, verbose=0).reshape(-1)\n",
        "    test_report = {\n",
        "        \"MAE\": float(mean_absolute_error(y_test, y_pred)),\n",
        "        \"RMSE\": float(math.sqrt(mean_squared_error(y_test, y_pred))),\n",
        "        \"R2\": float(r2_score(y_test, y_pred))\n",
        "    }\n",
        "else:\n",
        "    out = model.predict(X_test, verbose=0).reshape(-1)\n",
        "    y_hat = (1/(1+np.exp(-out)) >= 0.5).astype(int)  # binaria\n",
        "    test_report = {\"Accuracy\": float(accuracy_score(y_test, y_hat))}\n",
        "\n",
        "# Gráfica combinada de métricas + CSV de history\n",
        "plot_metrics_combined(history, problem_type, os.path.join(ARTIFACTS_DIR, \"metrics_overview.png\"), test_report=test_report)\n",
        "save_history_csv(history, os.path.join(ARTIFACTS_DIR, \"training_history.csv\"))\n",
        "\n",
        "# Guardar modelo y predicciones en test\n",
        "model.save(os.path.join(ARTIFACTS_DIR, \"modelo_tf_mina.keras\"))\n",
        "pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred if problem_type==\"regression\" else y_hat}) \\\n",
        "  .to_csv(os.path.join(ARTIFACTS_DIR, \"predicciones_test.csv\"), index=False)\n",
        "\n",
        "# =========================\n",
        "# GRÁFICAS MINERAS\n",
        "# =========================\n",
        "# Ton vs Ley (Cu/Au/Ag)\n",
        "for col, lbl in [(CU_COL, \"Cu (%)\"), (AU_COL, \"Au (g/t)\"), (AG_COL, \"Ag (g/t)\")]:\n",
        "    if col in df.columns and TON_COL in df.columns:\n",
        "        dplot = df[[col, TON_COL]].dropna()\n",
        "        if not dplot.empty:\n",
        "            safe_scatter(dplot[col].astype(float), dplot[TON_COL].astype(float),\n",
        "                         xlabel=f\"Ley {lbl}\", ylabel=\"Toneladas\",\n",
        "                         title=f\"Tonelada vs Ley ({lbl})\",\n",
        "                         outpath=os.path.join(ARTIFACTS_DIR, f\"Ton_vs_{col}.png\"))\n",
        "\n",
        "# NSR vs Ley / NSR vs Ton\n",
        "if NSR_COL in df.columns:\n",
        "    if CU_COL in df.columns:\n",
        "        dplot = df[[CU_COL, NSR_COL]].dropna()\n",
        "        if not dplot.empty:\n",
        "            safe_scatter(dplot[CU_COL].astype(float), dplot[NSR_COL].astype(float),\n",
        "                         xlabel=\"Ley Cu (%)\", ylabel=\"NSR (USD/t)\",\n",
        "                         title=\"NSR vs Ley Cu\",\n",
        "                         outpath=os.path.join(ARTIFACTS_DIR, \"NSR_vs_Cu.png\"))\n",
        "    if TON_COL in df.columns:\n",
        "        dplot = df[[TON_COL, NSR_COL]].dropna()\n",
        "        if not dplot.empty:\n",
        "            safe_scatter(dplot[TON_COL].astype(float), dplot[NSR_COL].astype(float),\n",
        "                         xlabel=\"Toneladas\", ylabel=\"NSR (USD/t)\",\n",
        "                         title=\"NSR vs Toneladas\",\n",
        "                         outpath=os.path.join(ARTIFACTS_DIR, \"NSR_vs_Ton.png\"))\n",
        "\n",
        "# Grado–Tonnage combinado (doble eje) + CSV + LÍNEA DE CUT-OFF\n",
        "for col, lbl in [(CU_COL, \"Cu (%)\"), (AU_COL, \"Au (g/t)\"), (AG_COL, \"Ag (g/t)\")]:\n",
        "    if col in df.columns and TON_COL in df.columns:\n",
        "        outprefix = os.path.join(ARTIFACTS_DIR, f\"grado_tonnage_{col}\")\n",
        "        curve = grado_tonnage_curve(df, col, TON_COL, steps=120)\n",
        "\n",
        "        # 1) Cálculo automático si hay NSR y Costo\n",
        "        vline = None\n",
        "        if NSR_COL in df.columns and COST_COL in df.columns:\n",
        "            vline = compute_economic_cutoff(df, grade_col=col, ton_col=TON_COL,\n",
        "                                            nsr_col=NSR_COL, cost_col=COST_COL, steps=200)\n",
        "        # 2) Si no hay auto, usar manual si existe\n",
        "        if vline is None and col in MANUAL_CUTOFFS:\n",
        "            vline = float(MANUAL_CUTOFFS[col])\n",
        "\n",
        "        plot_grado_tonnage_combined(curve, lbl, outprefix, vline=vline)\n",
        "\n",
        "# Reporte final\n",
        "print(\"\\n=== MÉTRICAS DE TEST ===\")\n",
        "print(json.dumps(test_report, indent=2, ensure_ascii=False))\n",
        "print(\"\\n✅ Resultados en 'artifacts/':\")\n",
        "print(\"- metrics_overview.png (métricas en UNA sola ventana)\")\n",
        "print(\"- training_history.csv\")\n",
        "print(\"- modelo_tf_mina.keras\")\n",
        "print(\"- predicciones_test.csv\")\n",
        "print(\"- Ton_vs_{Cu/Au/Ag}.png\")\n",
        "print(\"- NSR_vs_{Cu, Ton}.png (si hay NSR)\")\n",
        "print(\"- grado_tonnage_{Ley}_grado_tonnage_combined.png + _curve.csv (con línea de cut-off si aplica)\")\n"
      ]
    }
  ]
}